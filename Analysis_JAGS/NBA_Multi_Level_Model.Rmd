---
title: "NBA Multilevel Model"
author: "Yuka Chen"
date: "`r Sys.Date()`"
output: pdf_document
---
```{r setup, include=FALSE}
knitr::opts_chunk$set(
	message = FALSE,
	warning = FALSE,
	tidy = TRUE,
	error = TRUE,
	tidy.opts = list(width.cutoff = 70)
)
```
Last semester, I did a project with my friends about predicting NBA players' salary by their performance for Regression class. I want to try the same data with multiple level models to see if there's anything interesting different from the regular linear regression model.

- Previous project github repo: `<https://github.com/yjchen9596/NBA-Players-Salary-Prediction>`

For JAGS, I clean my data in a way that will work better. My github repo has more information about this project, as well as how I cleaned the data: 

- NBA JAGS repo `<https://github.com/yjchen9596/Multilevel_Model_NBAplayers_Salary>`

For the model, I use salary as y predictors and total points of the season by players as x. The data is a combination of NBA players salary history with 2016-2017 season and 2017-2018 season performance. To compare with previous project results, I kept the same two seasons for the data set as well.

As group level predictors, I categorized the player's age into three categories ('Young', 'Medium', 'Aged'). The second group level predictor is players' position.

I went with a multi-level varying intercept and varying slope model.

```{r message=FALSE, warning=FALSE, include=FALSE}
library(tidyverse)
library(rjags)
library(coda)
library(superdiag)
library(R2WinBUGS)
library(R2jags)
library(patchwork)
library(foreign)
library(arm)
library(MCMCpack)
library(runjags)
library(ggthemr)
library(mcmcplots)
library(superdiag)
library(ggmcmc)
library(knitr)
library(broom)
library(modelsummary)
```

```{r}
NBA <- read_csv("../Data/NBA.csv")
```

# Basic Simple Linear Regression

Allowing for the intercept to vary across NBA player_id. This is done using the lmer function, specifying varying intercepts using, position, age_group.

## lmer Model

```{r}
basic.linear.model <- lmer(salary ~  pts + (1 | position) + (1 | age_group), data = NBA, REML= FALSE)
modelsummary::msummary(list("Basic lmer Model" = basic.linear.model))
```

```{r}
VarCorr(basic.linear.model)
```

The model can be specified as below:

$$
\begin{aligned}
y_{i} \sim N(\alpha_{j[i]}+\beta_{j[i]}points,\sigma^2_y)
\end{aligned}
$$

$$
\begin{aligned}
\binom{\alpha_{j}}{\beta_{j}} = N(\ \binom{\gamma^{\alpha}_0+\gamma^\alpha_1group\_age_{j}+\gamma^\alpha_2position_j}{\gamma^{\beta}_0+\gamma^\beta_1group\_age_{j}+\gamma^\beta_2position_j}, \  \binom{\sigma^2_\alpha\ \ \ \ \ \ \rho\sigma_\alpha\sigma_\beta}{\rho\sigma_\alpha\ \ \ \ \ \ \ \ \ \sigma^2_\beta}\ )\ \ \  for \ \ j = 1, ...J
\end{aligned}
$$

The subscript j[i] denotes that the intercepts and slopes can vary by group. Modeling these group-level effects as a function of the group-level predictors age group and position; and allows there to be a correlation between the points and intercept parameters.

# JAGS - Simple Linear Regression

## Set Up Jags

This model predicts salary by points, allowing for the intercept to vary across NBA players, specifying varying intercepts using the players-specific indicator variable in dummies.

```{r include=FALSE}
set.seed(20221205)
```

```{r}
# Define variables
n <- length(NBA$salary)
unique.player <- unique(NBA$player_id)
J <- length(unique.player)
y <- NBA$salary 
x <- NBA$pts
player <- rep(NA, J)
for (i in 1:J) player[NBA$player_id == unique.player[i]] <- i
```

```{r echo=FALSE}
nba.jags.data <- list("n","J","y","player","x")
nba.jags.inits <- function(){
  list(a=rnorm(J), b=rnorm(1), mu.a=rnorm(1), sigma.y=runif(1), sigma.a=runif(1))}
nba.jags.parameters <- c("a", "b", "mu.a", "sigma.y", "sigma.a")
nba.jags <- function() { for (i in 1:n){
    y[i] ~ dnorm (y.hat[i], tau.y)
    y.hat[i] <- a[player[i]] + b*x[i]
  }
  b ~ dnorm (0, .0001)
  tau.y <- pow(sigma.y, -2)
  sigma.y ~ dunif (0, 100)
  for (j in 1:J){
    a[j] ~ dnorm (mu.a, tau.a)
} 
  mu.a ~ dnorm (0, .0001)
  tau.a <- pow(sigma.a, -2)
  sigma.a ~ dunif (0, 100)
}
write.model(nba.jags, "nba.jags.rjags")
nba.jags.out <- jags(data=nba.jags.data, inits=nba.jags.inits,
                      parameters.to.save=nba.jags.parameters,
                      model="nba.jags.rjags", n.chains=3, n.iter=2000, DIC=F)
nba.jags.out1a <- update(nba.jags.out, n.iter=10000)
```

##Summary 


*Because the original R Markdown went over 7 pages, so I set the first two jags model to only show output. Varying Intercept and Slopes models has the Jags code*


```{r }
# bb.mod.mcmc1 <- as.mcmc(nba.jags.out1a)
# mcmcplot(bb.mod.mcmc1)
knitr::kable(nba.jags.out1a$BUGSoutput$summary[c(1:2, 560:564), c(1, 2, 3, 7, 8, 9)], digits = 3)
```

## Check Convergence

```{r message=FALSE, warning=FALSE}
mcmc.nba.out1 <- as.mcmc(nba.jags.out1a, ariable.names=names(asap.jags.list)) 
nba.mcmc1.convergence <- superdiag(as.mcmc.list(mcmc.nba.out1), burnin=0)
# sink("nba.mcmc1.convergence2.txt")
# nba.mcmc1.convergence
```

The output was saved as txt file and I copied one of the diagnostic from the txt folder because whole output is too long.

+------------------------------+-----------+-----------+-------------+------------+
| The Raftery-Lewis diagnostic |           |           |             |            |
+:============================:+:=========:+:=========:+=============+============+
| Chain                        | 1:        |           |             |            |
+------------------------------+-----------+-----------+-------------+------------+
| Convergence eps              | 0.001     |           |             |            |
+------------------------------+-----------+-----------+-------------+------------+
| Quantile (q)                 | 0.025     |           |             |            |
+------------------------------+-----------+-----------+-------------+------------+
| Accuracy (r)                 | +/- 0.005 |           |             |            |
+------------------------------+-----------+-----------+-------------+------------+
| Probability (s)              | 0.95      |           |             |            |
+------------------------------+-----------+-----------+-------------+------------+
|                              | Burn-in   | Total     | Lower bound | Dependence |
+------------------------------+-----------+-----------+-------------+------------+
|                              | (M)       | (N)       | (Nmin)      | factor (I) |
+------------------------------+-----------+-----------+-------------+------------+
| a[1]                         | 2         | 3710      | 3746        | 0.990      |
+------------------------------+-----------+-----------+-------------+------------+
| a[10]                        | 2         | 3771      | 3746        | 1.010      |
+------------------------------+-----------+-----------+-------------+------------+
| a[100]                       | 2         | 3771      | 3746        | 1.010      |
+------------------------------+-----------+-----------+-------------+------------+
| a[101]                       | 2         | 3834      | 3746        | 1.020      |
+------------------------------+-----------+-----------+-------------+------------+

# JAGS - Varying Intercept

This model include group-level predictors (= players) for age group and position.

## Set Up Group Level Predictors

```{r warning=FALSE}
# Add group-level predictors
#### player position ####
age_group <- rep(NA, J)
for (i in unique(NBA$player_id)){age_group[i] <- unique(NBA$age_group[which(NBA$player_id == i)])}
age_group <- age_group[!is.na(age_group)]
#### age ####
position <- rep(NA, J)
for (i in unique(NBA$player_id)){position[i] <- unique(NBA$position[which(NBA$player_id == i)])}
position <- position[!is.na(position)]
```

## Set Up JAGS

```{r echo=FALSE}
nba.jags.data2 <- list("n","J","y","player","x", "age_group", "position")
nba.jags.inits2 <- function(){
  list(a=rnorm(J), b=rnorm(1), g0=rnorm(1), g1=rnorm(1), g2=rnorm(1),
       sigma.y=runif(1), sigma.a=runif(1))}
nba.jags.parameters2 <- c("a", "b", "g0", "g1", "g2", "sigma.y", "sigma.a")
nba.jags.M2 <- function() { 
  for (i in 1:n){
    y[i] ~ dnorm (y.hat[i], tau.y)
    y.hat[i] <- a[player[i]] + b*x[i]
  }
  b ~ dnorm (0, .0001)
  tau.y <- pow(sigma.y, -2)
  sigma.y ~ dunif (0, 100)
  
  for (j in 1:J){
    a[j] ~ dnorm (a.hat[j], tau.a)
    a.hat[j] <- g0 + g1*age_group[j] + g2*position[j]
}
  g0 ~ dnorm(0, 0.0001)
  g1 ~ dnorm(0, 0.0001)
  g2 ~ dnorm(0, 0.0001)
  tau.a <- pow(sigma.a, -2)
  sigma.a ~ dunif (0, 100)
}
write.model(nba.jags.M2, "nba.jags.M2.rjags")
nba.jags.out2 <- jags(data=nba.jags.data2, inits=nba.jags.inits2,
                      parameters.to.save=nba.jags.parameters2,
                      model="nba.jags.M2.rjags", n.chains=3, n.iter=2000, DIC=F)
```

## Summary

```{r message=FALSE, warning=FALSE}
nba.jags.out2a <- update(nba.jags.out2, n.iter=10000)
knitr::kable(nba.jags.out2a$BUGSoutput$summary[c(1:2, 561:566), c(1, 2, 3, 7, 8, 9)],
      digits = 3)
```

```{r}
# bb.mod.mcmc2 <- as.mcmc(nba.jags.out2a)
# mcmcplot(bb.mod.mcmc2)
```

## Check Convergence

```{r}
mcmc.nba.out2 <- as.mcmc(nba.jags.out2a, ariable.names=names(asap.jags.list)) 
nba.mcmc2.convergence <- superdiag(as.mcmc.list(mcmc.nba.out2), burnin=0)
# sink("nba.mcmc2.convergence.txt")
# nba.mcmc2.convergence
```

| The Raftery-Lewis diagnostic |           |       |
|------------------------------|-----------|-------|
| Chain 1                      |           |       |
| Convergence eps              | 0.001     |       |
| Quantile (q)                 | 0.025     |       |
| Accuracy (r)                 | +/- 0.005 |       |
| Probability (s)              | 0.95      |       |
|                              |           |       |
|                              | Burn-in   | Total |
|                              | (M)       | (N)   |
| a[1]                         | 2         | 3802  |
| a[10]                        | 2         | 3802  |

## Plot

```{r echo=FALSE, warning=FALSE, out.width = '50%',fig.align = 'center'}
ggthemr("solarized")
plotData = data.frame(pid = 1:length(unique(NBA$player_id)),
                      estM1 = nba.jags.out1a$BUGSoutput$summary[1:560,1],
                      lowerM1 = nba.jags.out1a$BUGSoutput$summary[1:560,3],
                      upperM1 = nba.jags.out1a$BUGSoutput$summary[1:560,7],
                      estM2 = nba.jags.out2a$BUGSoutput$summary[1:560,1],
                      lowerM2 = nba.jags.out2a$BUGSoutput$summary[1:560,3],
                      upperM2 = nba.jags.out2a$BUGSoutput$summary[1:560,7])

modelCol <- c("M1" = "dodgerblue", "M2" = "firebrick")
ggplot(data = plotData) +
  geom_hline(aes(yintercept =  nba.jags.out1a$BUGSoutput$summary[560,1], color = "M1"),linetype = "twodash", size = 1) +
  geom_hline(aes(yintercept =  nba.jags.out2a$BUGSoutput$summary[560,1], color = "M2"),linetype = "twodash", size = 1) +
  geom_pointrange(aes(x = pid + 0.5, y = estM1, ymin = lowerM1,ymax = upperM1, color = "M1"),position = position_dodge(width = 2/3),alpha = 0.5, shape = 1, fatten = 1, size = 1/2) +
  geom_pointrange(aes(x = pid - 0.5, y = estM2, ymin = lowerM2,ymax = upperM2, color = "M2"), position = position_dodge(width = 2/3),alpha = 0.6, shape = 1, fatten = 1, size = 1/2) +
  labs(y = "Estimated Intercept (Square Root)") +
  scale_color_manual(values = modelCol) +
  theme(legend.position="top",
        legend.title = element_blank(),
        axis.title.x = element_blank())
```

It is hard to see if there's any points influence on the players' salary. Instead of plotting all players, it will be easier to see the effects in individual players.

```{r message=FALSE, warning=FALSE, include=FALSE}
player_id_8 <- NBA |> filter(pts>10) |> 
  group_by(player_id) |> mutate(Len = n()) |> 
  filter(Len > 6) |> ungroup() |> 
  distinct(player_id)
selPid <- slice_sample(player_id_8, n = 8)
selPid <- selPid[["player_id"]]
selMod <- match(selPid, unique(NBA$player_id))
```

```{r include=FALSE}
## Select Random 8 People
ramdom_player <- data.frame(player_id = selPid,
                      intercept1 = nba.jags.out1a$BUGSoutput$summary[selMod,1],
                      slope1 = nba.jags.out1a$BUGSoutput$summary[564,1],
                      intercept2 = nba.jags.out2a$BUGSoutput$summary[selMod,1],
                      slope2 = nba.jags.out2a$BUGSoutput$summary[566,1])
```

```{r echo=FALSE, out.width = '50%', fig.align = 'center'}
ggthemr("solarized")
ggplot(NBA[NBA$player_id %in% selPid,]) + 
  ## 8 ramdom players - selection was done ny privious chuck which is hidden in pdf
  geom_point(alpha = 0.5, aes(x = pts, y = salary)) +
  geom_abline(data = ramdom_player, aes(intercept = intercept1, slope = slope1)) +
  geom_abline(data = ramdom_player, aes(intercept = intercept2, slope = slope2),
              linetype = "dashed") +
  labs(x = "Scored Points", y = "Salary") +
  facet_wrap(~player_id, nrow = 2)
```

It seems like there's a small increase in salary while players scored more points in seasons.

# JAGS - Varying Intercepts and Slopes

Varying intercepts and slopes Jags model allows intercepts and slopes to vary by player and to be estimated by the group-level predictors. Having varying intercepts and slopes allows us to model the multilevel correlation between intercepts and slopes.

## Set Up JAGS

```{r}
nba.jags.data3 <- list("n","J","y","player","x", "age_group", "position")
nba.jags.inits3 <- function(){list(B=array(rnorm(2*J), c(J,2)), g0a=rnorm(1), g1a=rnorm(1),  g2a=rnorm(1), g0b=rnorm(1), g1b=rnorm(1), g2b=rnorm(1), sigma.y=runif(1), sigma.a=runif(1))}
nba.jags.parameters3 <- c("a", "b", "sigma.y", "sigma.a", "rho", "g0a", "g1a","g2a", "g0b", "g1b", "g2b", "sigma.b")
nba.jags.M3 <- function(){ for(i in 1:n){
    y[i] ~ dnorm(y.hat[i], tau.y)
    y.hat[i] <- a[player[i]] + b[player[i]]*x[i]
  }
  tau.y <- pow(sigma.y, -2)
  sigma.y ~ dunif(0, 100) 
  for(j in 1:J){
    a[j] <- B[j,1]
    b[j] <- B[j,2]
    B[j, 1:2]  ~ dmnorm(B.hat[j,], Tau.B[,])
    B.hat[j,1] <- g0a + g1a*age_group[j] + g2a*position[j]
    B.hat[j,2] <- g0b + g1b*age_group[j] + g2b*position[j]
  }
  g0a  ~ dnorm(0, .0001)
  g1a  ~ dnorm(0, .0001)
  g2a  ~ dnorm(0, .0001)
  g0b  ~ dnorm(0, .0001)
  g1b  ~ dnorm(0, .0001)
  g2b  ~ dnorm(0, .0001)
  Tau.B[1:2,1:2] <- inverse(Sigma.B[,])
  Sigma.B[1,1] <- pow(sigma.a, 2)
  sigma.a  ~ dunif(0, 100)
  Sigma.B[2,2] <- pow(sigma.b, 2)
  sigma.b  ~ dunif(0, 100)
  Sigma.B[1,2] <- rho*sigma.a*sigma.b
  Sigma.B[2,1] <- Sigma.B[1,2]
  rho ~ dunif(-1,1)
}
write.model(nba.jags.M3, "nba.jags.M3.rjags")
# The number of iterations
M <- 10000
# This is the actual JAGS run
nba.jags.out3 <- jags(data=nba.jags.data3, inits=nba.jags.inits3,parameters.to.save=nba.jags.parameters3,model="nba.jags.M3.rjags", n.chains=3, n.iter=M, DIC=F)
```

## Summary

```{r}
# bb.mod.mcmc <- as.mcmc(nba.jags.out3)
# mcmcplot(bb.mod.mcmc)
knitr::kable(nba.jags.out3$BUGSoutput$summary[c(1:2,250:252,1120:1130),
                                              c(1, 2, 3, 7, 8, 9)], digits = 3,
             caption = "Varying Intercepts and Slopes")
```

The summary tables shows some of the player-specific intercepts and slopes. It also shows group-level regressions --- one for intercepts and one for slopes, and the estimated correlation $\rho$ between the intercept and slope parameter as well as the $\sigma$'s for individual level, intercepts, and slopes.

## Check Convergence

```{r}
mcmc.nba.out3 <- as.mcmc(nba.jags.out3, ariable.names=names(asap.jags.list)) 
nba.mcmc3.convergence <- superdiag(as.mcmc.list(mcmc.nba.out3), burnin=0)
# sink("nba.mcmc3.convergence.txt")
# nba.mcmc3.convergence
```

| The Raftery-Lewis diagnostic  	|  	|
|---	|---	|
| Chain 1: 	|  	|
| Convergence eps = 	| 0.001 	|
| Quantile (q) = 	| 0.025 	|
| Accuracy (r) = 	| +/- 0.005 	|
| Probability (s) = 	| 0.95 	|

| Chain 2: 	|  	|
|---	|---	|
| Convergence eps  	| 2e-04 	|
| Quantile (q)  	| 0.05 	|
| Accuracy (r)  	| +/- 0.005 	|
| Probability (s)  	| 0.9 	|


| Chain 3:        |           |
|-----------------|-----------|
| Convergence eps | 0.0025    |
| Quantile (q)    | 0.05      |
| Accuracy (r)    | +/- 0.001 |
| Probability (s) | 0.9       |

- Chain 1: You need a sample size of at least 3746 with these values of q, r and s 
- Chain 2: You need a sample size of at least 5141 with these values of q, r and s
- Chain 3: You need a sample size of at least 128514 with these values of q, r and s

## Plot
```{r include=FALSE}
plotData3 <- data.frame(player_id = selPid,
                      intercept1 = nba.jags.out2a$BUGSoutput$summary[selMod,1],
                      slope1 = nba.jags.out2a$BUGSoutput$summary[566,1],
                      intercept2 = nba.jags.out3$BUGSoutput$summary[selMod,1],
                      slope2 = nba.jags.out3$BUGSoutput$summary[566+selMod,1])
modelCol <- c("M1" = "dodgerblue", "M2" = "firebrick")
modelLine <- c("M1" = "solid", "M2" = "dashed")
```

```{r echo=FALSE, out.width = '50%',fig.align = 'center'}
ggthemr("solarized")
ggplot(NBA[NBA$player_id %in% selPid,]) +
  geom_point(alpha = 0.5, aes(x = pts, y = salary))+
    facet_wrap(~player_id, nrow = 2)+
  geom_abline(data = plotData3, aes(intercept = intercept1, slope = slope1,color = "M1", linetype = "M1")) +
  geom_abline(data = plotData3, aes(intercept = intercept2, slope = slope2,color = "M2", linetype = "M2"))+
  scale_color_manual(name="Model", values = modelCol,labels = c("Varying Intercepts", "Varying Intercepts and Slopes")) +
   scale_linetype_manual(name="Model", values = modelLine,labels = c("Varying Intercepts", "Varying Intercepts and Slopes"))+
  labs(x = "Points", y = "Salary") +
  theme(legend.position="top") 

```

```{r, out.width = '50%',fig.align = 'center'}
ggplot(NBA[NBA$player_id %in% selPid,]) +
  geom_point(alpha = 0.5, aes(x = pts, y = salary)) +
  geom_abline(data = plotData3, aes(intercept = intercept1, slope = slope1,
                                    color = "M1", linetype = "M1")) +
  geom_abline(data = plotData3, aes(intercept = intercept2, slope = slope2,
                                    color = "M2", linetype = "M2")) +
  scale_color_manual(name="Model", values = modelCol,
                     labels = c("Varying Intercepts", "Varying Intercepts and Slopes")) +
  scale_linetype_manual(name="Model", values = modelLine,
                        labels = c("Varying Intercepts", "Varying Intercepts and Slopes")) +
  labs(x = "Points", y = "Salary") +
  theme(legend.position="top") +
  facet_wrap(~player_id, nrow = 2)
```

There's dramatic differences of fit between varying intercept and varying intercept and slopes. From the plot, I think Varying Intercepts and Slopes model seems better in terms of fitting the data.

Next, we plot all the estimates against group-level predictors to see if the estimates are different in terms of different levels of group-level predictors.
